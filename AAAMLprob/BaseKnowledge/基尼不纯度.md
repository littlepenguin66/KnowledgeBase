基尼不纯度（Gini Impurity）是[[AAAMLprob/BaseKnowledge/决策树]]算法中用来衡量数据集纯度的一种指标。它衡量的是从数据集中随机选取两个样本，其类别标签不一致的概率。一个数据集的基尼不纯度越高，表示数据集中的样本属于不同类别的概率越大，即数据集的纯度越低。基尼不纯度的计算公式如下：
$$
 G(i) = 1 - \sum_{k=1}^{n} p_{k}(i)^2 
$$
其中，\( G(i) \) 表示节点 \( i \) 的基尼不纯度，\( n \) 是类别的数量，\( p_{k}(i) \) 是节点 \( i \) 中类别 \( k \) 的相对频率。
当我们在构建决策树时，会选择那些能够最大程度减少基尼不纯度的特征来作为节点的划分依据。具体来说，对于给定的特征 \( A \) 和可能的划分点 \( v \)，我们会计算划分前后的基尼不纯度变化，即基尼增益（Gini Gain），并选择能够最大化基尼增益的划分点作为节点的划分依据。基尼增益的计算公式如下：
$$
 Gini\ Gain(A, v) = G(i) - \left( \frac{|i_L|}{|i|} G(i_L) + \frac{|i_R|}{|i|} G(i_R) \right) 
$$
其中，$G(i)$是节点 $i$ 的基尼不纯度，$i_L$ 和 $i_R$ 分别是划分后得到的左子节点和右子节点，$|i|$、$|i_L|$ 和 $|i_R|$ 分别是节点 $i$、$i_L$ 和 $i_R$ 中的样本数量。
通过这种方式，我们可以逐步构建出一棵能够有效分类数据的决策树。
``` python
import numpy as np

def gini_impurity(labels):
    """
    计算基尼不纯度
    :param labels: 样本类别标签的列表或数组
    :return: 基尼不纯度
    """
    # 计算每个类别的相对频率
    unique_classes, class_counts = np.unique(labels, return_counts=True)
    class_probabilities = class_counts / len(labels)
    
    # 计算基尼不纯度
    gini = 1 - np.sum(class_probabilities**2)
    
    return gini

# 示例数据集
labels = np.array([0, 0, 1, 1, 2, 2, 2])

# 计算基尼不纯度
gini = gini_impurity(labels)
print(f"基尼不纯度: {gini}")
```